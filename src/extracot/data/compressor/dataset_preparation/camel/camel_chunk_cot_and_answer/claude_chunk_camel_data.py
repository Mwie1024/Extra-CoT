import re
from typing import List, Tuple, Dict, Any
from dataclasses import dataclass
try:
    from transformers import LongformerTokenizerFast
    TOKENIZER_CLASS = LongformerTokenizerFast
except ImportError:
    from transformers import LongformerTokenizer
    TOKENIZER_CLASS = LongformerTokenizer

# å¯¼å…¥ç”¨æˆ·çš„å…¬å¼è¯†åˆ«å‡½æ•°
from math_tokenizer_no_space import latex_spans, find_formula_spans_nolatex, merge_spans

@dataclass
class Sentence:
    content: str
    start: int
    end: int
    tokens: int = 0
    contains_math: bool = False

class SmartCoTChunker:
    def __init__(self, tokenizer_name: str = 'allenai/longformer-base-4096'):
        """åˆå§‹åŒ–chunker"""
        self.tokenizer = TOKENIZER_CLASS.from_pretrained(tokenizer_name)
        self.target_tokens = 512
        self.safe_window_start = 450
        self.min_merge_tokens = 128

    def get_token_count(self, text: str) -> int:
        """è·å–æ–‡æœ¬çš„tokenæ•°é‡"""
        if not text.strip():
            return 0
        return len(self.tokenizer.tokenize(text))

    def get_all_math_spans(self, text: str) -> List[Tuple[int, int]]:
        """è·å–æ‰€æœ‰æ•°å­¦å…¬å¼çš„ä½ç½®åŒºé—´"""
        latex_math_spans = latex_spans(text)
        nolatex_math_spans = find_formula_spans_nolatex(text)
        return merge_spans(latex_math_spans + nolatex_math_spans)

    def is_position_in_math(self, position: int, math_spans: List[Tuple[int, int]]) -> bool:
        """æ£€æŸ¥æŸä¸ªä½ç½®æ˜¯å¦åœ¨æ•°å­¦å…¬å¼å†…"""
        for start, end in math_spans:
            if start <= position < end:
                return True
        return False

    def split_into_sentences(self, text: str) -> List[Sentence]:
        """å°†æ–‡æœ¬æŒ‰å¥å­åˆ†å‰²ï¼Œä½†é¿å¼€æ•°å­¦å…¬å¼å†…çš„æ ‡ç‚¹ç¬¦å·"""
        math_spans = self.get_all_math_spans(text)
        
        # å¥å­ç»“æŸæ ‡è®°çš„æ­£åˆ™æ¨¡å¼
        sentence_endings = [
            r'[ã€‚.!?ï¼ï¼Ÿ]',  # ä¸­è‹±æ–‡å¥å·ã€æ„Ÿå¹å·ã€é—®å·
            r'âœ“',            # éªŒè¯ç¬¦å·
            r'[ï¼›;]',        # åˆ†å·
        ]
        
        sentences = []
        current_start = 0
        
        # é€å­—ç¬¦æ‰«æï¼Œå¯»æ‰¾å¥å­è¾¹ç•Œ
        i = 0
        while i < len(text):
            char = text[i]
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ½œåœ¨çš„å¥å­ç»“æŸç¬¦
            is_potential_ending = False
            for pattern in sentence_endings:
                if re.match(pattern, char):
                    is_potential_ending = True
                    break
            
            if is_potential_ending:
                # æ£€æŸ¥è¿™ä¸ªä½ç½®æ˜¯å¦åœ¨æ•°å­¦å…¬å¼å†…
                if not self.is_position_in_math(i, math_spans):
                    # è¿™æ˜¯ä¸€ä¸ªçœŸæ­£çš„å¥å­è¾¹ç•Œ
                    # å¥å­åœ¨ç»“æŸç¬¦å¤„ç»“æŸ
                    sentence_content = text[current_start:i+1].strip()
                    if sentence_content:
                        # æ£€æŸ¥è¿™ä¸ªå¥å­æ˜¯å¦åŒ…å«æ•°å­¦å…¬å¼
                        contains_math = any(
                            start < current_start + len(sentence_content) and end > current_start
                            for start, end in math_spans
                        )
                        
                        sentence = Sentence(
                            content=sentence_content,
                            start=current_start,
                            end=i+1,
                            tokens=self.get_token_count(sentence_content),
                            contains_math=contains_math
                        )
                        sentences.append(sentence)
                    
                    # è·³è¿‡å¥å­ç»“æŸç¬¦åçš„ç©ºç™½å­—ç¬¦ï¼Œå¼€å§‹ä¸‹ä¸€ä¸ªå¥å­
                    next_start = i + 1
                    while next_start < len(text) and text[next_start].isspace():
                        next_start += 1
                    
                    current_start = next_start
                    i = next_start
                    continue
            
            i += 1
        
        # å¤„ç†æœ€åçš„æ–‡æœ¬ç‰‡æ®µ
        if current_start < len(text):
            remaining_content = text[current_start:].strip()
            if remaining_content:
                contains_math = any(
                    start < len(text) and end > current_start
                    for start, end in math_spans
                )
                
                sentence = Sentence(
                    content=remaining_content,
                    start=current_start,
                    end=len(text),
                    tokens=self.get_token_count(remaining_content),
                    contains_math=contains_math
                )
                sentences.append(sentence)
        
        return sentences

    def merge_short_sentences(self, sentences: List[Sentence]) -> List[Sentence]:
        """åˆå¹¶è¿‡çŸ­çš„å¥å­ç‰‡æ®µ"""
        if not sentences:
            return []
        
        merged = []
        current_group = [sentences[0]]
        current_tokens = sentences[0].tokens
        
        for sentence in sentences[1:]:
            # å¦‚æœå½“å‰ç»„åˆçš„tokenæ•°è¿˜å¾ˆå°‘ï¼Œç»§ç»­åˆå¹¶
            if current_tokens < 50 and len(current_group) < 3:
                current_group.append(sentence)
                current_tokens += sentence.tokens
            else:
                # åˆ›å»ºåˆå¹¶åçš„å¥å­
                if len(current_group) == 1:
                    merged.append(current_group[0])
                else:
                    combined_content = ' '.join(s.content for s in current_group)
                    combined_sentence = Sentence(
                        content=combined_content,
                        start=current_group[0].start,
                        end=current_group[-1].end,
                        tokens=current_tokens,
                        contains_math=any(s.contains_math for s in current_group)
                    )
                    merged.append(combined_sentence)
                
                # å¼€å§‹æ–°çš„ç»„
                current_group = [sentence]
                current_tokens = sentence.tokens
        
        # å¤„ç†æœ€åä¸€ç»„
        if current_group:
            if len(current_group) == 1:
                merged.append(current_group[0])
            else:
                combined_content = ' '.join(s.content for s in current_group)
                combined_sentence = Sentence(
                    content=combined_content,
                    start=current_group[0].start,
                    end=current_group[-1].end,
                    tokens=sum(s.tokens for s in current_group),
                    contains_math=any(s.contains_math for s in current_group)
                )
                merged.append(combined_sentence)
        
        return merged

    def calculate_sentence_split_score(self, sentence: Sentence, current_tokens: int) -> float:
        """è®¡ç®—åœ¨æŸä¸ªå¥å­ååˆ†å‰²çš„è¯„åˆ†"""
        # è·ç¦»ç›®æ ‡tokenæ•°çš„æƒ©ç½š
        distance_penalty = 1.0 - abs(current_tokens - self.target_tokens) / 62.0
        distance_penalty = max(0.1, distance_penalty)
        
        # æ ¹æ®å¥å­ç‰¹å¾ç¡®å®šä¼˜å…ˆçº§æƒé‡
        content = sentence.content.strip()
        
        # é€»è¾‘ç»“è®ºå¥å­ï¼ˆå› æ­¤ã€æ‰€ä»¥ç­‰ï¼‰
        if re.search(r'(å› æ­¤|æ‰€ä»¥|ç»¼ä¸Š|æ€»ä¹‹|Therefore|Thus|Hence)[^ã€‚]*[ã€‚.!?ï¼ï¼Ÿâœ“]\s*$', content):
            priority_weight = 10.0
        
        # å®Œæ•´å¥å­ç»“å°¾ï¼ˆå¥å·ã€æ„Ÿå¹å·ã€é—®å·ï¼‰
        elif re.search(r'[ã€‚.!?ï¼ï¼Ÿ]\s*$', content):
            priority_weight = 9.0
        
        # éªŒè¯ç¬¦å·ç»“å°¾
        elif re.search(r'âœ“\s*$', content):
            priority_weight = 9.0
        
        # åŒ…å«æ•°å­¦å…¬å¼çš„å¥å­
        elif sentence.contains_math:
            priority_weight = 8.0
        
        # åˆ†å·ç»“å°¾
        elif re.search(r'[ï¼›;]\s*$', content):
            priority_weight = 6.0
        
        # å†’å·ç»“å°¾
        elif re.search(r'[ï¼š:]\s*$', content):
            priority_weight = 4.0
        
        # æ™®é€šå¥å­
        else:
            priority_weight = 5.0
        
        return priority_weight * distance_penalty

    def smart_chunk_split(self, text: str) -> List[str]:
        """åŸºäºå¥å­è¾¹ç•Œçš„æ™ºèƒ½åˆ†å‰²"""
        # 1. å°†æ–‡æœ¬åˆ†å‰²ä¸ºå¥å­
        sentences = self.split_into_sentences(text)
        
        # 2. åˆå¹¶è¿‡çŸ­çš„å¥å­ç‰‡æ®µ
        sentences = self.merge_short_sentences(sentences)
        
        if not sentences:
            return [text] if text.strip() else []
        
        # 3. åŸºäºå¥å­è¾¹ç•Œè¿›è¡Œchunkåˆ†å‰²
        chunks = []
        current_chunk_sentences = []
        current_tokens = 0
        
        i = 0
        while i < len(sentences):
            sentence = sentences[i]
            
            # æ£€æŸ¥æ·»åŠ è¿™ä¸ªå¥å­æ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶
            if (current_tokens + sentence.tokens > self.target_tokens and 
                current_tokens >= self.safe_window_start and 
                current_chunk_sentences):
                
                # éœ€è¦åˆ†å‰²ï¼Œå¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
                best_split_idx = self.find_best_sentence_split(
                    current_chunk_sentences, sentences[i:i+5]  # å‘å‰çœ‹5ä¸ªå¥å­
                )
                
                # åˆ›å»ºchunk
                chunk_content = ' '.join(s.content for s in current_chunk_sentences[:best_split_idx+1])
                chunks.append(chunk_content)
                
                # æ›´æ–°çŠ¶æ€
                remaining_sentences = current_chunk_sentences[best_split_idx+1:]
                current_chunk_sentences = remaining_sentences + [sentence]
                current_tokens = sum(s.tokens for s in current_chunk_sentences)
                i += 1
                
            else:
                # ç›´æ¥æ·»åŠ å¥å­
                current_chunk_sentences.append(sentence)
                current_tokens += sentence.tokens
                i += 1
        
        # å¤„ç†æœ€åçš„å¥å­
        if current_chunk_sentences:
            chunk_content = ' '.join(s.content for s in current_chunk_sentences)
            chunks.append(chunk_content)
        
        return self.merge_last_chunk_if_needed(chunks)

    def find_best_sentence_split(self, current_sentences: List[Sentence], 
                                upcoming_sentences: List[Sentence]) -> int:
        """åœ¨å½“å‰å¥å­ä¸­æ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹"""
        if not current_sentences:
            return 0
        
        best_idx = len(current_sentences) - 1  # é»˜è®¤åœ¨æœ€ååˆ†å‰²
        best_score = -1
        cumulative_tokens = 0
        
        # è®¡ç®—åˆ°æ¯ä¸ªå¥å­çš„ç´¯ç§¯tokenæ•°ï¼Œå¯»æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
        for i, sentence in enumerate(current_sentences):
            cumulative_tokens += sentence.tokens
            
            if cumulative_tokens >= self.safe_window_start:
                score = self.calculate_sentence_split_score(sentence, cumulative_tokens)
                if score > best_score:
                    best_score = score
                    best_idx = i
        
        # ä¹Ÿè€ƒè™‘åŠ ä¸Šæ¥ä¸‹æ¥çš„å¥å­æ˜¯å¦æœ‰æ›´å¥½çš„åˆ†å‰²ç‚¹
        for i, sentence in enumerate(upcoming_sentences):
            cumulative_tokens += sentence.tokens
            if cumulative_tokens > self.target_tokens + 100:  # ç¡¬é™åˆ¶
                break
            
            score = self.calculate_sentence_split_score(sentence, cumulative_tokens)
            if score > best_score:
                best_score = score
                best_idx = len(current_sentences) + i
        
        # ç¡®ä¿ä¸è¶…å‡ºcurrent_sentencesçš„èŒƒå›´
        return min(best_idx, len(current_sentences) - 1)

    def merge_last_chunk_if_needed(self, chunks: List[str]) -> List[str]:
        """å¤„ç†æœ€åchunkçš„åˆå¹¶"""
        if len(chunks) <= 1:
            return chunks
        
        last_chunk_tokens = self.get_token_count(chunks[-1])
        
        if last_chunk_tokens < self.min_merge_tokens:
            merged_content = chunks[-2] + ' ' + chunks[-1]
            merged_tokens = self.get_token_count(merged_content)
            
            if merged_tokens <= self.target_tokens + 50:
                return chunks[:-2] + [merged_content]
        
        return chunks

    def analyze_sentence_structure(self, text: str) -> str:
        """åˆ†æå¥å­ç»“æ„ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
        sentences = self.split_into_sentences(text)
        sentences = self.merge_short_sentences(sentences)
        
        output = ["=== SENTENCE ANALYSIS ==="]
        cumulative_tokens = 0
        
        for i, sentence in enumerate(sentences):
            cumulative_tokens += sentence.tokens
            math_indicator = "ğŸ“" if sentence.contains_math else "ğŸ“"
            
            # è®¡ç®—åˆ†å‰²è¯„åˆ†
            score = self.calculate_sentence_split_score(sentence, cumulative_tokens)
            
            output.append(f"{i:2d}: {math_indicator} [{cumulative_tokens:3d}t] Score:{score:5.2f}")
            output.append(f"     Content: {sentence.content[:100]}")
            
            if cumulative_tokens >= self.safe_window_start:
                output.append(f"     >>> ğŸ”„ In split consideration zone")
        
        return "\n".join(output)

    def validate_chunks(self, original_text: str, chunks: List[str]) -> Dict[str, Any]:
        """éªŒè¯chunkç»“æœ"""
        validation_result = {
            'valid': True,
            'errors': [],
            'warnings': [],
            'stats': {
                'total_chunks': len(chunks),
                'token_distribution': [],
                'math_formula_integrity': True
            }
        }
        
        # Tokenæ•°é‡ç»Ÿè®¡
        for i, chunk in enumerate(chunks):
            tokens = self.get_token_count(chunk)
            validation_result['stats']['token_distribution'].append({
                'chunk_id': i,
                'tokens': tokens,
                'chars': len(chunk)
            })
            
            if tokens > self.target_tokens + 50:
                validation_result['warnings'].append(f'Chunk {i} exceeds target: {tokens} tokens')
        
        # æ£€æŸ¥æ•°å­¦å…¬å¼å®Œæ•´æ€§
        try:
            original_math_spans = self.get_all_math_spans(original_text)
            
            for start, end in original_math_spans:
                formula = original_text[start:end]
                found_complete = False
                
                for chunk in chunks:
                    if formula in chunk:
                        found_complete = True
                        break
                
                if not found_complete:
                    validation_result['valid'] = False
                    validation_result['errors'].append(f'Math formula broken: {formula[:50]}...')
                    validation_result['stats']['math_formula_integrity'] = False
        except Exception as e:
            validation_result['warnings'].append(f'Math validation error: {str(e)}')
        
        return validation_result

    def chunk_text(self, text: str, validate: bool = True) -> Dict[str, Any]:
        """å®Œæ•´çš„chunkingæµç¨‹"""
        chunks = self.smart_chunk_split(text)
        
        result = {
            'chunks': chunks,
            'chunk_count': len(chunks),
        }
        
        if validate:
            result['validation'] = self.validate_chunks(text, chunks)
        
        return result

    def read_jsonl_file(self, file_path: str) -> List[Dict[str, Any]]:
        """è¯»å–JSONLæ–‡ä»¶"""
        import json
        samples = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:
                        try:
                            sample = json.loads(line)
                            samples.append(sample)
                        except json.JSONDecodeError as e:
                            print(f"è­¦å‘Š: ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
            
            print(f"æˆåŠŸè¯»å– {len(samples)} ä¸ªæ ·æœ¬")
            return samples
            
        except FileNotFoundError:
            print(f"é”™è¯¯: æ–‡ä»¶ {file_path} æœªæ‰¾åˆ°")
            return []
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return []

    def process_all_samples(self, samples: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å¤„ç†æ‰€æœ‰æ ·æœ¬çš„CoTæ–‡æœ¬"""
        results = []
        
        for i, sample in enumerate(samples):
            print(f"å¤„ç†æ ·æœ¬ {i+1}/{len(samples)}: {sample.get('id', f'sample_{i+1}')}")
            
            try:
                # æå–CoTæ–‡æœ¬
                cot_text = sample.get('answer', sample.get('cot', sample.get('reasoning', '')))
                
                if not cot_text:
                    print(f"  è­¦å‘Š: æ ·æœ¬ {sample.get('id', f'sample_{i+1}')} æ²¡æœ‰æ‰¾åˆ°CoTæ–‡æœ¬")
                    results.append({
                        'id': sample.get('id', f'sample_{i+1}'),
                        'question': sample.get('question', ''),
                        'original_cot': '',
                        'error': 'No CoT text found',
                        'chunks': [],
                        'chunk_count': 0
                    })
                    continue
                
                # æ‰§è¡Œåˆ†å—å¤„ç†
                result = self.chunk_text(cot_text, validate=True)
                
                # æ„å»ºç»“æœå¯¹è±¡
                processed_result = {
                    'id': sample.get('id', f'sample_{i+1}'),
                    'question': sample.get('question', ''),
                    'original_cot': cot_text,
                    'chunks': result['chunks'],
                    'chunk_count': result['chunk_count'],
                    'validation': result.get('validation', {}),
                    'token_distribution': result['validation']['stats']['token_distribution'] if 'validation' in result else []
                }
                
                results.append(processed_result)
                print(f"  âœ… æˆåŠŸå¤„ç†ï¼Œç”Ÿæˆ {result['chunk_count']} ä¸ªchunks")
                
            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {str(e)}")
                results.append({
                    'id': sample.get('id', f'sample_{i+1}'),
                    'question': sample.get('question', ''),
                    'original_cot': sample.get('answer', sample.get('cot', sample.get('reasoning', ''))),
                    'error': str(e),
                    'chunks': [],
                    'chunk_count': 0
                })
        
        return results

    def save_processed_results(self, processed_results: List[Dict[str, Any]], output_path: str):
        """ä¿å­˜å¤„ç†åçš„ç»“æœåˆ°JSONLæ–‡ä»¶"""
        import json
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for result in processed_results:
                # åˆ›å»ºè¾“å‡ºæ ¼å¼
                output_item = {
                    'id': result['id'],
                    'question': result['question'],
                    'original_cot': result['original_cot'],
                    'chunks': result['chunks'],
                    'chunk_count': result['chunk_count'],
                    'processing_status': 'success' if 'error' not in result else 'failed',
                }
                
                if 'error' in result:
                    output_item['error'] = result['error']
                
                if 'validation' in result:
                    output_item['validation_info'] = {
                        'valid': result['validation']['valid'],
                        'token_distribution': result['token_distribution'],
                        'math_formula_integrity': result['validation']['stats']['math_formula_integrity']
                    }
                
                f.write(json.dumps(output_item, ensure_ascii=False) + '\n')

    def generate_token_statistics(self, processed_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆtokenç»Ÿè®¡ä¿¡æ¯"""
        successful_results = [r for r in processed_results if 'error' not in r]
        
        if not successful_results:
            return {"error": "No successful results to analyze"}
        
        # æ”¶é›†æ‰€æœ‰chunkçš„tokenæ•°
        all_chunk_tokens = []
        for result in successful_results:
            for chunk_info in result.get('token_distribution', []):
                all_chunk_tokens.append(chunk_info['tokens'])
        
        if not all_chunk_tokens:
            return {"error": "No token data available"}
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        import statistics
        stats = {
            "total_chunks": len(all_chunk_tokens),
            "mean_tokens": statistics.mean(all_chunk_tokens),
            "median_tokens": statistics.median(all_chunk_tokens),
            "min_tokens": min(all_chunk_tokens),
            "max_tokens": max(all_chunk_tokens),
            "std_tokens": statistics.stdev(all_chunk_tokens) if len(all_chunk_tokens) > 1 else 0,
        }
        
        # tokenåˆ†å¸ƒåŒºé—´
        stats["token_ranges"] = {
            "under_400": sum(1 for t in all_chunk_tokens if t < 400),
            "400_to_450": sum(1 for t in all_chunk_tokens if 400 <= t < 450),
            "450_to_512": sum(1 for t in all_chunk_tokens if 450 <= t <= 512),
            "over_512": sum(1 for t in all_chunk_tokens if t > 512),
        }
        
        return stats

    def analyze_chunk_distribution(self, processed_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æchunkåˆ†å¸ƒæƒ…å†µ"""
        successful_results = [r for r in processed_results if 'error' not in r]
        
        chunk_counts = [r['chunk_count'] for r in successful_results]
        
        if not chunk_counts:
            return {"error": "No successful results to analyze"}
        
        import statistics
        distribution = {
            "samples_with_1_chunk": sum(1 for c in chunk_counts if c == 1),
            "samples_with_2_chunks": sum(1 for c in chunk_counts if c == 2),
            "samples_with_3_chunks": sum(1 for c in chunk_counts if c == 3),
            "samples_with_4_plus_chunks": sum(1 for c in chunk_counts if c >= 4),
            "max_chunks_in_sample": max(chunk_counts),
            "average_chunks_per_sample": statistics.mean(chunk_counts),
        }
        
        return distribution

    def save_statistics(self, stats: Dict[str, Any], stats_path: str):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
        import json
        
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

    def get_current_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().isoformat()

    def visualize_chunks(self, text: str, chunks: List[str]) -> str:
        """å¯è§†åŒ–chunkåˆ†å‰²ç»“æœ"""
        output = []
        for i, chunk in enumerate(chunks):
            tokens = self.get_token_count(chunk)
            
            # æ˜¾ç¤ºchunkçš„å‰åè¾¹ç•Œ
            start_text = chunk[:60].replace('\n', '\\n')
            end_text = chunk[-60:].replace('\n', '\\n')
            
            output.append(f"\n{'='*15} CHUNK {i+1} ({tokens} tokens) {'='*15}")
            output.append(f"å¼€å§‹: {start_text}...")
            output.append(f"ç»“æŸ: ...{end_text}")
            output.append("="*60)
        
        return "\n".join(output)
        """å¯è§†åŒ–chunkåˆ†å‰²ç»“æœ"""
        output = []
        for i, chunk in enumerate(chunks):
            tokens = self.get_token_count(chunk)
            
            # æ˜¾ç¤ºchunkçš„å‰åè¾¹ç•Œ
            start_text = chunk[:60].replace('\n', '\\n')
            end_text = chunk[-60:].replace('\n', '\\n')
            
            output.append(f"\n{'='*15} CHUNK {i+1} ({tokens} tokens) {'='*15}")
            output.append(f"å¼€å§‹: {start_text}...")
            output.append(f"ç»“æŸ: ...{end_text}")
            output.append("="*60)
        
        return "\n".join(output)


# ä½¿ç”¨ç¤ºä¾‹
def example_usage():
    # æŒ‡å®šJSONLæ–‡ä»¶è·¯å¾„
    jsonl_file_path = "/Users/mwie/User/Data/Code/CoT Language/CoT_Language/dataset_preparation/camel/camel_chunk/test_chunk.jsonl"
    
    try:
        chunker = SmartCoTChunker()
        
        # è¯»å–JSONLæ–‡ä»¶
        print("=== å¼€å§‹è¯»å–JSONLæ–‡ä»¶ ===")
        samples = chunker.read_jsonl_file(jsonl_file_path)
        
        if not samples:
            print("æ²¡æœ‰è¯»å–åˆ°ä»»ä½•æ ·æœ¬ï¼Œç¨‹åºé€€å‡º")
            return
        
        # å¤„ç†æ‰€æœ‰æ ·æœ¬
        print("\n=== å¼€å§‹å¤„ç†æ‰€æœ‰æ ·æœ¬çš„CoTæ–‡æœ¬ ===")
        original_cot_list = chunker.process_all_samples(samples)
        
        # æ˜¾ç¤ºå¤„ç†ç»“æœç»Ÿè®¡
        print("\n=== å¤„ç†ç»“æœç»Ÿè®¡ ===")
        total_samples = len(original_cot_list)
        successful_samples = sum(1 for item in original_cot_list if 'error' not in item)
        failed_samples = total_samples - successful_samples
        total_chunks = sum(item.get('chunk_count', 0) for item in original_cot_list if 'error' not in item)
        
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"æˆåŠŸå¤„ç†: {successful_samples}")
        print(f"å¤„ç†å¤±è´¥: {failed_samples}")
        print(f"æ€»chunkæ•°: {total_chunks}")
        
        # æ˜¾ç¤ºæ¯ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
        print("\n=== æ ·æœ¬è¯¦ç»†ä¿¡æ¯ ===")
        for i, item in enumerate(original_cot_list):
            print(f"\næ ·æœ¬ {i+1}: {item['id']}")
            print(f"é—®é¢˜: {item['question'][:100]}...")
            
            if 'error' in item:
                print(f"çŠ¶æ€: å¤„ç†å¤±è´¥ - {item['error']}")
            else:
                print(f"çŠ¶æ€: å¤„ç†æˆåŠŸ")
                print(f"Chunkæ•°é‡: {item['chunk_count']}")
                print(f"Tokenåˆ†å¸ƒ: {[chunk['tokens'] for chunk in item['token_distribution']]}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªchunkçš„é¢„è§ˆ
        print("\n=== Chunké¢„è§ˆ ===")
        for i, item in enumerate(original_cot_list[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬
            if 'error' not in item and item['chunks']:
                print(f"\n--- æ ·æœ¬ {item['id']} çš„Chunks ---")
                for j, chunk in enumerate(item['chunks']):
                    tokens = chunker.get_token_count(chunk)
                    preview = chunk[:100].replace('\n', '\\n')
                    print(f"  Chunk {j+1} ({tokens} tokens): {preview}...")
        
        # ä¿å­˜å¤„ç†ç»“æœ
        print("\n=== å¼€å§‹ä¿å­˜ç»“æœ ===")
        output_file_path = "/Users/mwie/User/Data/Code/CoT Language/CoT_Language/dataset_preparation/camel/camel_chunk/chunked_cot_output.jsonl"
        stats_file_path = "/Users/mwie/User/Data/Code/CoT Language/CoT_Language/dataset_preparation/camel/camel_chunk/chunking_stats.json"
        
        try:
            # ä¿å­˜å¤„ç†åçš„æ•°æ®
            chunker.save_processed_results(original_cot_list, output_file_path)
            
            # ç”Ÿæˆå¹¶ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
            stats = {
                "processing_summary": {
                    "total_samples": total_samples,
                    "successful_samples": successful_samples,
                    "failed_samples": failed_samples,
                    "total_chunks": total_chunks,
                    "average_chunks_per_sample": total_chunks / successful_samples if successful_samples > 0 else 0
                },
                "token_statistics": chunker.generate_token_statistics(original_cot_list),
                "chunk_distribution": chunker.analyze_chunk_distribution(original_cot_list),
                "processing_timestamp": chunker.get_current_timestamp()
            }
            
            chunker.save_statistics(stats, stats_file_path)
            
            print(f"âœ… å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: {output_file_path}")
            print(f"âœ… ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜åˆ°: {stats_file_path}")
            
        except Exception as save_error:
            print(f"âŒ ä¿å­˜è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(save_error)}")
            import traceback
            traceback.print_exc()
        
        print(f"\n=== å¤„ç†å®Œæˆ ===")
        print(f"æ‰€æœ‰ç»“æœå·²ä¿å­˜åœ¨ original_cot_list å˜é‡ä¸­")
        print(f"original_cot_list åŒ…å« {len(original_cot_list)} ä¸ªæ ·æœ¬çš„å¤„ç†ç»“æœ")
        
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    example_usage()