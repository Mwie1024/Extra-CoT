#!/usr/bin/env python3
"""
æ£€æŸ¥CHUNKED_RESULT.jsonlæ–‡ä»¶ä¸­æ¯ä¸ªæ ·æœ¬çš„validation_infoå…ƒç´ ä¸­çš„token_distributionä¸­tokensçš„åˆ†å¸ƒæƒ…å†µ
"""

import json
import statistics
from collections import Counter
import matplotlib.pyplot as plt
import numpy as np

def analyze_token_distribution(file_path):
    """
    åˆ†æJSONLæ–‡ä»¶ä¸­æ¯ä¸ªæ ·æœ¬çš„tokenåˆ†å¸ƒæƒ…å†µ
    """
    token_counts = []
    chunk_counts = []
    sample_stats = []
    high_token_samples = []  # å­˜å‚¨tokenæ•°è¶…è¿‡1000çš„æ ·æœ¬ä¿¡æ¯
    min_token_samples = []
    
    print("æ­£åœ¨è¯»å–æ–‡ä»¶...")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                data = json.loads(line.strip())
                
                # æ£€æŸ¥æ˜¯å¦æœ‰validation_infoå’Œtoken_distribution
                if 'validation_info' in data and 'token_distribution' in data['validation_info']:
                    token_dist = data['validation_info']['token_distribution']
                    
                    # æå–æ¯ä¸ªchunkçš„tokenæ•°é‡
                    sample_tokens = [chunk['tokens'] for chunk in token_dist]
                    token_counts.extend(sample_tokens)
                    chunk_counts.append(len(sample_tokens))
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰tokenæ•°è¶…è¿‡1000çš„chunk
                    max_tokens_in_sample = max(sample_tokens) if sample_tokens else 0
                    if max_tokens_in_sample > 1000:
                        high_token_samples.append({
                            'sample_id': data.get('id', f'line_{line_num}'),
                            'max_tokens': max_tokens_in_sample,
                            'token_distribution': sample_tokens,
                            'chunk_count': len(sample_tokens)
                        })
                    
                    min_tokens_in_sample = min(sample_tokens) if sample_tokens else 999999
                    if min_tokens_in_sample < 200:
                        min_token_samples.append({
                            'sample_id': data.get('id', f'line_{line_num}'),
                            'min_tokens': min_tokens_in_sample,
                            'token_distribution': sample_tokens,
                            'chunk_count': len(sample_tokens)
                        })
                    
                    # è®°å½•æ¯ä¸ªæ ·æœ¬çš„ç»Ÿè®¡ä¿¡æ¯
                    sample_stats.append({
                        'sample_id': data.get('id', f'line_{line_num}'),
                        'chunk_count': len(sample_tokens),
                        'total_tokens': sum(sample_tokens),
                        'min_tokens': min(sample_tokens) if sample_tokens else 0,
                        'max_tokens': max_tokens_in_sample,
                        'avg_tokens': statistics.mean(sample_tokens) if sample_tokens else 0,
                        'token_distribution': sample_tokens
                    })
                else:
                    print(f"è­¦å‘Š: ç¬¬{line_num}è¡Œç¼ºå°‘validation_infoæˆ–token_distribution")
                    
            except json.JSONDecodeError as e:
                print(f"é”™è¯¯: ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
            except Exception as e:
                print(f"é”™è¯¯: ç¬¬{line_num}è¡Œå¤„ç†å¤±è´¥: {e}")
    
    return token_counts, chunk_counts, sample_stats, high_token_samples, min_token_samples

def print_statistics(token_counts, chunk_counts, sample_stats, high_token_samples, min_token_samples):
    """
    æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    """
    print("\n" + "="*60)
    print("TOKEN DISTRIBUTION ANALYSIS REPORT")
    print("="*60)
    
    # æ€»ä½“tokenç»Ÿè®¡
    print(f"\nğŸ“Š Overall Token Statistics:")
    print(f"  Total samples: {len(sample_stats)}")
    print(f"  Total chunks: {len(token_counts)}")
    print(f"  Total tokens: {sum(token_counts):,}")
    print(f"  Average tokens per chunk: {statistics.mean(token_counts):.2f}")
    print(f"  Median tokens: {statistics.median(token_counts):.2f}")
    print(f"  Token standard deviation: {statistics.stdev(token_counts):.2f}")
    print(f"  Minimum tokens: {min(token_counts)}")
    print(f"  Maximum tokens: {max(token_counts)}")
    
    # Chunkæ•°é‡ç»Ÿè®¡
    print(f"\nğŸ“¦ Chunk Count Statistics:")
    chunk_counter = Counter(chunk_counts)
    print(f"  Average chunks per sample: {statistics.mean(chunk_counts):.2f}")
    print(f"  Median chunks: {statistics.median(chunk_counts):.2f}")
    print(f"  Minimum chunks: {min(chunk_counts)}")
    print(f"  Maximum chunks: {max(chunk_counts)}")
    print(f"  Chunk count distribution:")
    for chunk_num in sorted(chunk_counter.keys()):
        print(f"    {chunk_num} chunks: {chunk_counter[chunk_num]} samples")
    
    # Tokenæ•°é‡åŒºé—´ç»Ÿè®¡
    print(f"\nğŸ¯ Token Range Statistics:")
    token_ranges = [
        (0, 100, "0-100"),
        (100, 200, "100-200"),
        (200, 300, "200-300"),
        (300, 400, "300-400"),
        (400, 500, "400-500"),
        (500, 600, "500-600"),
        (600, 700, "600-700"),
        (700, 800, "700-800"),
        (800, 900, "800-900"),
        (900, 1000, "900-1000"),
        (1000, float('inf'), "1000+")
    ]
    
    for min_val, max_val, label in token_ranges:
        count = sum(1 for tokens in token_counts if min_val <= tokens < max_val)
        percentage = (count / len(token_counts)) * 100
        print(f"  {label:>8}: {count:>6} chunks ({percentage:>5.1f}%)")
    
    # æ ·æœ¬ç»Ÿè®¡
    print(f"\nğŸ“ˆ Sample-level Statistics:")
    total_tokens_per_sample = [s['total_tokens'] for s in sample_stats]
    print(f"  Average total tokens per sample: {statistics.mean(total_tokens_per_sample):.2f}")
    print(f"  Median total tokens per sample: {statistics.median(total_tokens_per_sample):.2f}")
    print(f"  Minimum total tokens per sample: {min(total_tokens_per_sample)}")
    print(f"  Maximum total tokens per sample: {max(total_tokens_per_sample)}")
    
    # é«˜tokenæ•°æ ·æœ¬ä¿¡æ¯
    if high_token_samples:
        print(f"\nğŸš¨ Samples with tokens > 1000:")
        print(f"  Found {len(high_token_samples)} samples with chunks containing > 1000 tokens:")
        for sample in high_token_samples:
            print(f"    Sample ID: {sample['sample_id']}")
            print(f"    Max tokens: {sample['max_tokens']}")
            print(f"    Chunk count: {sample['chunk_count']}")
            print(f"    Token distribution: {sample['token_distribution']}")
            print()
    else:
        print(f"\nâœ… No samples found with tokens > 1000")

    # ä½tokenæ•°æ ·æœ¬ä¿¡æ¯
    if min_token_samples:
        print(f"\nğŸš¨ Samples with tokens < 1000:")
        print(f"  Found {len(min_token_samples)} samples with chunks containing < 1000 tokens:")
        for sample in min_token_samples:
            print(f"    Sample ID: {sample['sample_id']}")
            print(f"    Min tokens: {sample['min_tokens']}")
            print(f"    Chunk count: {sample['chunk_count']}")
            print(f"    Token distribution: {sample['token_distribution']}")
            print()
    else:
        print(f"\nâœ… No samples found with tokens < 1000")

def print_sample_details(sample_stats, num_samples=5):
    """
    æ‰“å°å‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
    """
    print(f"\nğŸ” Details of first {num_samples} samples:")
    print("-" * 80)
    
    for i, sample in enumerate(sample_stats[:num_samples]):
        print(f"\nSample {i+1}: {sample['sample_id']}")
        print(f"  Chunk count: {sample['chunk_count']}")
        print(f"  Total tokens: {sample['total_tokens']}")
        print(f"  Average tokens: {sample['avg_tokens']:.2f}")
        print(f"  Token range: {sample['min_tokens']} - {sample['max_tokens']}")
        print(f"  Token distribution: {sample['token_distribution']}")

def create_visualizations(token_counts, chunk_counts, sample_stats):
    """
    åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    """
    print(f"\nğŸ“Š Generating visualization charts...")
    
    # åˆ›å»ºå­å›¾
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Token Distribution Analysis', fontsize=16, fontweight='bold')
    
    # 1. Tokenæ•°é‡ç›´æ–¹å›¾
    axes[0, 0].hist(token_counts, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].set_title('Chunk Token Count Distribution')
    axes[0, 0].set_xlabel('Token Count')
    axes[0, 0].set_ylabel('Frequency')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. Chunkæ•°é‡åˆ†å¸ƒ
    chunk_counter = Counter(chunk_counts)
    chunk_nums = sorted(chunk_counter.keys())
    chunk_freqs = [chunk_counter[num] for num in chunk_nums]
    axes[0, 1].bar(chunk_nums, chunk_freqs, alpha=0.7, color='lightgreen', edgecolor='black')
    axes[0, 1].set_title('Chunk Count Distribution per Sample')
    axes[0, 1].set_xlabel('Number of Chunks')
    axes[0, 1].set_ylabel('Number of Samples')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. æ ·æœ¬æ€»tokenæ•°åˆ†å¸ƒ
    total_tokens_per_sample = [s['total_tokens'] for s in sample_stats]
    axes[1, 0].hist(total_tokens_per_sample, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')
    axes[1, 0].set_title('Total Token Count Distribution per Sample')
    axes[1, 0].set_xlabel('Total Token Count')
    axes[1, 0].set_ylabel('Number of Samples')
    axes[1, 0].grid(True, alpha=0.3)
    
    # 4. Tokenæ•°é‡ç®±çº¿å›¾
    axes[1, 1].boxplot(token_counts, vert=True, patch_artist=True, 
                       boxprops=dict(facecolor='lightyellow', alpha=0.7))
    axes[1, 1].set_title('Token Count Box Plot')
    axes[1, 1].set_ylabel('Token Count')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ä¿å­˜å›¾è¡¨
    output_file = '/Users/mwie/User/Data/Code/CoT Language/CoT_Language/token_distribution_analysis.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print(f"Chart saved to: {output_file}")
    
    plt.show()

def main():
    """
    ä¸»å‡½æ•°
    """
    file_path = '/Users/mwie/User/Data/Code/CoT Language/CoT_Language/dataset_preparation/camel/camel_chunk/CHUNKED_RESULT.jsonl'
    
    print("Starting token distribution analysis...")
    print(f"File path: {file_path}")
    
    # åˆ†ææ•°æ®
    token_counts, chunk_counts, sample_stats, high_token_samples, min_token_samples = analyze_token_distribution(file_path)
    
    if not token_counts:
        print("Error: No valid token data found")
        return
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print_statistics(token_counts, chunk_counts, sample_stats, high_token_samples, min_token_samples)
    
    # æ‰“å°æ ·æœ¬è¯¦æƒ…
    print_sample_details(sample_stats, num_samples=5)
    
    # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
    try:
        create_visualizations(token_counts, chunk_counts, sample_stats)
    except ImportError:
        print("\nNote: matplotlib not installed, skipping visualization chart generation")
    except Exception as e:
        print(f"\nVisualization chart generation failed: {e}")
    
    print(f"\nâœ… Analysis completed!")

if __name__ == "__main__":
    main()
